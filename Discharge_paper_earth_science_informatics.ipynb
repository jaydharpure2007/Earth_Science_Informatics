{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95818f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define file path\n",
    "file_path = r'C:\\Users\\dharpure.1\\Downloads\\Discharge paper\\M3.xlsx'\n",
    "\n",
    "# Define file save path\n",
    "path_save = r'C:\\Users\\dharpure.1\\Downloads\\Discharge paper\\Model 2\\M3\\\\'\n",
    "if not os.path.exists(path_save):\n",
    "    os.makedirs(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def normalize(df):\n",
    "    # Normalize data from scratch\n",
    "    normalized_df = pd.DataFrame()\n",
    "    min_max_value = {}\n",
    "    for column in df.columns:\n",
    "        if column != 'Date':\n",
    "            col_min = df[column].min()\n",
    "            col_max = df[column].max()\n",
    "            normalized_df[column] = ((df[column] - col_min) / (col_max - col_min)) * 2 - 1\n",
    "        else:\n",
    "            normalized_df[column] = df[column]\n",
    "        if column == 'Discharge': \n",
    "            min_max_value[column] = {'min': col_min, 'max': col_max}\n",
    "    \n",
    "    return normalized_df, min_max_value\n",
    "\n",
    "def de_normalize(discharge, min_max_value):\n",
    "    discharge_min = min_max_value['Discharge']['min']\n",
    "    discharge_max = min_max_value['Discharge']['max']\n",
    "    denormalized_df = (discharge + 1) / 2 * (discharge_max - discharge_min) + discharge_min\n",
    "    return np.round(denormalized_df, 2)\n",
    "\n",
    "def calculate_matrix(observed, predicted):\n",
    "    RMSE = np.sqrt(mean_squared_error(observed, predicted))\n",
    "    mean_observed = np.mean(observed)\n",
    "    NSE = 1 - (np.sum((predicted - observed)**2) / np.sum((observed - mean_observed)**2))\n",
    "    r = pearsonr(observed, predicted)\n",
    "    return {'RMSE': round(RMSE, 3), 'NSE': round(NSE, 3), 'R': round(r[0], 3)}\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "df_data = pd.read_excel(file_path)\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52760f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_number = int(input ('Please setect input scenario data using number, i.e., M1 indicates 1, M2 indicates 2 and so on'))\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "def cross_corr_figure(df_data):\n",
    "    index_to_stop = df_data.index[df_data['Discharge'].isna()].tolist()[0]\n",
    "    model_df = df_data.loc[:index_to_stop-1].dropna()\n",
    "    df =  model_df.iloc[:, 1:]\n",
    "    c = df.shape[1]\n",
    "    title_data = ['(a) Ta (\\N{DEGREE SIGN}C)', '(b) Precipitation (mm)', '(c) Snowfall (mm)', '(d) ET (mm)', '(e) SCA (%)', '(f) Discharge (m\\u00b3/s)']\n",
    "    \n",
    "    # Global figure properties\n",
    "    fontsize = 8\n",
    "    legend_fontsize = 9\n",
    "    linewidth = 0.3\n",
    "    markersize = 3\n",
    "    markeredgewidth = 0.5\n",
    "    dpi = 300\n",
    "\n",
    "    params = {\n",
    "        'axes.labelsize': fontsize,\n",
    "        'axes.titlesize': fontsize,\n",
    "        'font.size': fontsize,\n",
    "        'figure.titlesize': fontsize,\n",
    "        'legend.fontsize': legend_fontsize,\n",
    "        'xtick.labelsize': fontsize,\n",
    "        'ytick.labelsize': fontsize,\n",
    "        'axes.linewidth': linewidth,\n",
    "        'savefig.dpi': dpi,\n",
    "        'font.family': 'sans-serif',\n",
    "        'figure.facecolor': 'w',\n",
    "    }\n",
    "\n",
    "    mpl.rcParams.update(params)\n",
    "    ccf_data = {}\n",
    "    slice_ccf = 11\n",
    "    for col in range(c):\n",
    "        discharge = df.iloc[:, -1]\n",
    "        var = df.iloc[:, col]\n",
    "        ccf_data[title_data[col]] = sm.tsa.ccf(discharge, var)[:slice_ccf]\n",
    "    x = [i for i in range(slice_ccf)]\n",
    "    fig, axs = plt.subplots(nrows=int(c/2), ncols=2, figsize=(10, 8), constrained_layout=True)\n",
    "    for ax, title, var_ccf in zip(axs.flat, list(ccf_data.keys()), list(ccf_data.values())):\n",
    "        ax.set_title(title)\n",
    "        ax.plot(x, var_ccf, 'o', color=\"r\", ls='-', ms=4, markevery=None)\n",
    "        ax.set_xlabel(\"Lag time (days)\")\n",
    "        ax.set_ylabel(\"Cross_corr\")\n",
    "    \n",
    "    return ccf_data\n",
    "\n",
    "def build_lagged_features(s, lag=2, dropna=True, col_name='None'):\n",
    "    if type(s) is pd.Series:\n",
    "        the_range = range(lag+1)\n",
    "        res = pd.concat([s.shift(i+1) for i in the_range], axis=1)\n",
    "        res.columns = [f'{col_name}_{i}' for i in the_range]\n",
    "    else:\n",
    "        print('Only works for DataFrame or Series')\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "data = list(cross_corr_figure(df_data).values())\n",
    "df =  df_data.iloc[:, 1:]\n",
    "\n",
    "# Please select max_corr_index as per the scenario dataset\n",
    "# Index represents [Ta, Pt, Snowfall, Et, SCA, discharge]\n",
    "if(model_number==1):\n",
    "    max_corr_index = [2, 1, 1, 0]  # for M1\n",
    "elif(model_number==2):\n",
    "    max_corr_index = [2, 1, 1, 0, 0]  # for M2\n",
    "elif(model_number==3):\n",
    "    max_corr_index = [2, 1, 1, 0, 1]  # for M3   \n",
    "elif(model_number==4):\n",
    "    max_corr_index = [2, 1, 1, 0, 0,1]  # for M4\n",
    "    \n",
    "new_dict = {}\n",
    "for corr, col_name in zip(max_corr_index, df.columns.to_list()):\n",
    "    res = build_lagged_features(df[col_name], lag=corr, dropna=False, col_name=col_name)\n",
    "    new_dict[col_name] = res\n",
    "    \n",
    "df_features = pd.DataFrame()\n",
    "df_features['Date'] = df_data.iloc[:, 0]\n",
    "for key_var in new_dict.keys():\n",
    "    for key_lag in new_dict[key_var].keys():\n",
    "        df_features[key_lag] = new_dict[key_var][key_lag]\n",
    "df_features['Discharge'] = df_data.iloc[:, -1]\n",
    "\n",
    "col_list = list(df_data.columns[1:])\n",
    "dict_lag_corr = {}\n",
    "for row in range(np.array(data).shape[0]):\n",
    "    dict_lag_corr[col_list[row]] = np.array(data)[row, :]\n",
    "corr_df = pd.DataFrame(dict_lag_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681094bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34fed5",
   "metadata": {},
   "source": [
    "## Data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_stop = df_features.index[df_features['Discharge'].isna()].tolist()[0]\n",
    "\n",
    "# Select rows up to the index_to_stop\n",
    "model_df = df_features.loc[:index_to_stop-1].dropna()\n",
    "\n",
    "# Normalize the model data\n",
    "norm_model, min_max_value = normalize(model_df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data = norm_model[norm_model['Date'] <= '2010-09-30']\n",
    "test_data = norm_model[norm_model['Date'] >= '2010-10-01']\n",
    "\n",
    "# Prepare data for prediction\n",
    "predict_df = df_features.iloc[index_to_stop:, :-1]\n",
    "norm_predict, _ = normalize(predict_df)\n",
    "\n",
    "# Separate features and target for training\n",
    "X_train = train_data.iloc[:, 1:-1]  # Features\n",
    "y_train = train_data.iloc[:, -1]    # Target\n",
    "y_train_denor = de_normalize(y_train, min_max_value)\n",
    "\n",
    "# Separate features and target for testing\n",
    "X_test = test_data.iloc[:, 1:-1]\n",
    "y_test = de_normalize(test_data.iloc[:, -1], min_max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9da2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e508639",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22434628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=42, max_features='log2')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the train set\n",
    "y_pred_train = de_normalize(model.predict(X_train), min_max_value)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = de_normalize(model.predict(X_test), min_max_value)\n",
    "\n",
    "# Define the path for saving results\n",
    "path = path_save + 'RandomForest\\\\'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Save train data results\n",
    "train_result = {\n",
    "    'Date': pd.to_datetime(train_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_train_denor,\n",
    "    'Predicted': y_pred_train\n",
    "}\n",
    "df_train_result = pd.DataFrame(train_result)\n",
    "df_train_result.to_excel(path + 'train_result_rf.xlsx', index=False)\n",
    "print(\"Train results saved in Excel\")\n",
    "\n",
    "# Save test data results\n",
    "test_result = {\n",
    "    'Date': pd.to_datetime(test_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_test,\n",
    "    'Predicted': y_pred_test\n",
    "}\n",
    "df_test_result = pd.DataFrame(test_result)\n",
    "df_test_result.to_excel(path + '\\\\Test_result_rf.xlsx', index=False)\n",
    "print(\"Test results saved in Excel\")\n",
    "\n",
    "# Calculate and save metrics\n",
    "columns = ['RMSE', 'NSE', 'R']\n",
    "matrics_rf = pd.DataFrame(columns=columns)\n",
    "train_df_rf = pd.DataFrame([calculate_matrix(y_train_denor, y_pred_train)], index=['Train'])\n",
    "test_df_rf = pd.DataFrame([calculate_matrix(y_test, y_pred_test)], index=['Test'])\n",
    "matrics_rf = pd.concat([train_df_rf, test_df_rf], ignore_index=False)\n",
    "matrics_rf.to_excel(path + 'Matrics_rf.xlsx')\n",
    "print(\"Metrics results saved in Excel\")\n",
    "\n",
    "# Save feature importance\n",
    "dict_rf = {\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}\n",
    "dict_df_rf = pd.DataFrame(dict_rf)\n",
    "dict_df_rf.to_excel(path + 'Feature_importance_rf.xlsx')\n",
    "print(\"Feature importance results saved in Excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c76059",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrics_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044859c",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ff250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Support Vector Machine (SVM) model\n",
    "model = SVR(kernel='rbf', C=10, epsilon=0.1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the train set\n",
    "y_pred_train = de_normalize(model.predict(X_train), min_max_value)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = de_normalize(model.predict(X_test), min_max_value)\n",
    "\n",
    "# Define the path for saving results\n",
    "path = path_save + 'SVM\\\\'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Save train data results\n",
    "train_result = {\n",
    "    'Date': pd.to_datetime(train_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_train_denor,\n",
    "    'Predicted': y_pred_train\n",
    "}\n",
    "df_train_result = pd.DataFrame(train_result)\n",
    "df_train_result.to_excel(path + 'train_result_svm.xlsx', index=False)\n",
    "print(\"Train results saved in Excel\")\n",
    "\n",
    "# Save test data results\n",
    "test_result = {\n",
    "    'Date': pd.to_datetime(test_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_test,\n",
    "    'Predicted': y_pred_test\n",
    "}\n",
    "df_test_result = pd.DataFrame(test_result)\n",
    "df_test_result.to_excel(path + '\\\\Test_result_svm.xlsx', index=False)\n",
    "print(\"Test results saved in Excel\")\n",
    "\n",
    "# Calculate and save metrics\n",
    "columns = ['RMSE', 'NSE', 'R']\n",
    "matrics_svm = pd.DataFrame(columns=columns)\n",
    "train_df_svm = pd.DataFrame([calculate_matrix(y_train_denor, y_pred_train)], index=['Train'])\n",
    "test_df_svm = pd.DataFrame([calculate_matrix(y_test, y_pred_test)], index=['Test'])\n",
    "matrics_svm = pd.concat([train_df_svm, test_df_svm], ignore_index=False)\n",
    "matrics_svm.to_excel(path + 'Matrics_svm.xlsx')\n",
    "print(\"Metrics results saved in Excel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5446a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrics_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281e07b",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data and model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.dropout1 = nn.Dropout(p=0.10)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(p=0.10)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNetwork(X_train.shape[1], 1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor).squeeze().float()\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Convert predictions back to original scale if needed\n",
    "y_pred_test_denor = de_normalize(torch.tensor(y_pred_test), min_max_value)\n",
    "\n",
    "# Save results to Excel\n",
    "path = path_save + 'ANN\\\\'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Test data save\n",
    "test_result = {\n",
    "    'Date': pd.to_datetime(test_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_test,\n",
    "    'Predicted': y_pred_test_denor.flatten()\n",
    "}\n",
    "df_test_result = pd.DataFrame(test_result)\n",
    "df_test_result.to_excel(os.path.join(path, 'Test_result_ANN.xlsx'), index=False)\n",
    "print(\"Test results saved in Excel\")\n",
    "\n",
    "# Predictions on training set\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).cpu().numpy()\n",
    "\n",
    "# Convert predictions back to original scale if needed\n",
    "y_pred_train_denor = de_normalize(y_pred_train, min_max_value)\n",
    "\n",
    "# Save results to Excel\n",
    "train_result = {\n",
    "    'Date': pd.to_datetime(train_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_train_denor,\n",
    "    'Predicted': y_pred_train_denor.flatten()\n",
    "}\n",
    "df_train_result = pd.DataFrame(train_result)\n",
    "df_train_result.to_excel(os.path.join(path, 'train_result_ANN.xlsx'), index=False)\n",
    "print(\"Train results saved in Excel\")\n",
    "\n",
    "# Metrics calculation and save\n",
    "train_metrics = calculate_matrix(y_train_denor, y_pred_train_denor.flatten())\n",
    "test_metrics = calculate_matrix(y_test, y_pred_test_denor.numpy().flatten())\n",
    "\n",
    "matrics_ANN = pd.DataFrame([train_metrics, test_metrics], index=['Train', 'Test'])\n",
    "matrics_ANN.to_excel(os.path.join(path, 'Matrics_ANN.xlsx'))\n",
    "print(\"Metrics results saved in Excel\")\n",
    "\n",
    "torch.save(model, path+str(model_number)+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3301e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrics_ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0638eb7",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to prepare sequences for LSTM\n",
    "def series_to_supervised_gap(sequences, n_steps_in=1, n_steps_out=1):\n",
    "    X = list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        seq_x = sequences[i:end_ix]\n",
    "        out_end_ix = end_ix + n_steps_out - 1\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        X.append(seq_x)\n",
    "    return np.array(X)\n",
    "\n",
    "# Your data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to tensors and move to device\n",
    "X_train_tensor = torch.tensor(series_to_supervised_gap(X_train, n_steps_in=time_step), dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(series_to_supervised_gap(X_test, n_steps_in=time_step), dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the Bidirectional LSTM model\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rates):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(input_size if i == 0 else hidden_sizes[i - 1] * 2,  # Adjusted input size for bidirectional\n",
    "                    hidden_size,\n",
    "                    num_layers=2,\n",
    "                    dropout=dropout_rates[i],\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True)\n",
    "            for i, (hidden_size, dropout_rate) in enumerate(zip(hidden_sizes, dropout_rates))\n",
    "        ])\n",
    "        self.linear = nn.Linear(hidden_sizes[-1] * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            x, _ = lstm_layer(x)\n",
    "        out = self.linear(x[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize model parameters and optimizer\n",
    "input_size = X_train_tensor.shape[-1]\n",
    "output_size = 1\n",
    "hidden_sizes = [128, 64]\n",
    "dropout_rates = [0.1, 0.1]\n",
    "\n",
    "model = BidirectionalLSTM(input_size, hidden_sizes, output_size, dropout_rates).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "# Training loop\n",
    "nepochs = 250\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs.flatten(), y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{nepochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Convert predictions back to the original scale if needed\n",
    "y_pred_test_denor = de_normalize(torch.tensor(y_pred_test.flatten()), min_max_value)\n",
    "\n",
    "# Save results to Excel\n",
    "path = path_save + 'BiLSTM\\\\'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Test data save\n",
    "test_result = {\n",
    "    'Date': pd.to_datetime(test_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_test,\n",
    "    'Predicted': y_pred_test_denor.numpy()\n",
    "}\n",
    "df_test_result = pd.DataFrame(test_result)\n",
    "df_test_result.to_excel(os.path.join(path, 'Test_result_LSTM_Bidirectional.xlsx'), index=False)\n",
    "print(\"Test results saved in Excel\")\n",
    "\n",
    "# Make predictions on the training set\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).cpu().numpy()\n",
    "\n",
    "# Convert predictions back to the original scale if needed\n",
    "y_pred_train_dnor = de_normalize(torch.tensor(y_pred_train.flatten()), min_max_value)\n",
    "\n",
    "# Train data save\n",
    "train_result = {\n",
    "    'Date': pd.to_datetime(train_data['Date']).dt.strftime('%Y-%m-%d'),\n",
    "    'Observed': y_train_denor,\n",
    "    'Predicted': y_pred_train_dnor.numpy()\n",
    "}\n",
    "df_train_result = pd.DataFrame(train_result)\n",
    "df_train_result.to_excel(os.path.join(path, 'train_result_LSTM_Bidirectional.xlsx'), index=False)\n",
    "print(\"Train results saved in Excel\")\n",
    "\n",
    "# Metrics save\n",
    "train_metrics = calculate_matrix(y_train_denor, y_pred_train_dnor.numpy())\n",
    "test_metrics = calculate_matrix(y_test, y_pred_test_denor.numpy())\n",
    "\n",
    "matrics_LSTM = pd.DataFrame([train_metrics, test_metrics], index=['Train', 'Test'])\n",
    "matrics_LSTM.to_excel(os.path.join(path, 'Matrics_LSTM_Bidirectional.xlsx'))\n",
    "print(\"Metrics results saved in Excel\")\n",
    "\n",
    "torch.save(model, path+str(model_number)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrics_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
